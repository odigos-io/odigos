image:
  tag: ''
imagePullSecrets: []

# By default, images are pulled from odigos registry at `registry.odigos.io`
# If you use custom or internal registry to serve in your cluster, you can set the imagePrefix to your registry.
# For example, if you set imagePrefix to `myregistry.io/odigos`, the images will be pulled from `myregistry.io/odigos/odigos-<component>:<tag>`
imagePrefix:

# namespaces list not to show in odigos ui
# set by default: odigos-system, kube-system, local-path-storage, istio-system, linkerd, kube-node-lease, odigos-system
# you can add additional namespaces to ignore by adding them to the list
ignoredNamespaces:

# container names to never instrument
# useful for sidecars which are not interesting to be instrumented
# set by default: istio-proxy, vault-agent, filebeat, linkerd-proxy, fluentd, akeyless-init
# you can add additional container names to ignore by adding them to the list
ignoredContainers:

# Name of the cluster, will be used to identify this cluster in the centralized backend
clusterName: ''

userInstrumentationEnvs:
  # Configuration for OpenTelemetry (OTEL) agents instrumentation.
  # These settings enable and configure OTEL agents for programming languages supported by Odigos.
  # See the official OTEL documentation for language-specific configuration details:
  # https://opentelemetry.io/docs/zero-code/
  # Example:
  # languages:
  #   java:
  #     enabled: true
  #     env:
  #       OTEL_INSTRUMENTATION_COMMON_EXPERIMENTAL_VIEW_TELEMETRY_ENABLED: "true"
  # Note: For eBPF-based distributions, exporting and batching cannot be configured here, as they are managed by the Odiglet.
  # Warning: This is an advanced feature. Only modify these settings if you are familiar with OTEL and its implications.
  languages:
    java:
      enabled: false
      env: {}
    python:
      enabled: false
      env: {}
    nodejs:
      enabled: false
      env: {}
    go:
      enabled: false
      env: {}
    dotnet:
      enabled: false
      env: {}
    php:
      enabled: false
      env: {}

collectorGateway:
  # the memory request for the cluster gateway collector deployment.
  # it will be embedded in the deployment as a resource request
  # of the form "memory: <value>Mi".
  # default value is 500Mi
  requestMemoryMiB: 500
  # the memory limit for the cluster gateway collector deployment.
  # it will be embedded in the deployment as a resource limit
  # of the form "memory: <value>Mi".
  # default value is 625Mi
  limitMemoryMiB: 625

  # the CPU request for the cluster gateway collector deployment.
  # it will be embedded in the deployment as a resource request
  # of the form "cpu: <value>m".
  # default value is 500m
  requestCPUm: 500
  # the CPU limit for the cluster gateway collector deployment.
  # it will be embedded in the deployment as a resource limit
  # of the form "cpu: <value>m".
  # default value is 1000m
  limitCPUm: 1000

  # The number of replicas for the cluster gateway collector deployment.
  # Also uses in MinReplicas the HPA config.
  minReplicas: 1
  # The maxReplicas in the HPA config.
  maxReplicas: 10

  # sets the "limit_mib" parameter in the memory limiter configuration for the collector gateway.
  # it is the hard limit after which a force garbage collection will be performed.
  # if not set, it will be 50Mi below the memory request.
  memoryLimiterLimitMiB: 450
  # sets the "spike_limit_mib" parameter in the memory limiter configuration for the collector gateway.
  # note that this is not the processor soft limit, but the diff in MiB between the hard limit and the soft limit.
  # if not specified, this value will be set to 20% of the hard limit (so the soft limit will be 80% of the hard limit).
  memoryLimiterSpikeLimitMiB: 90
  # the GOMEMLIMIT environment variable value for the collector gateway deployment.
  # this is when go runtime will start garbage collection.
  # if not specified, it will be set to 80% of the hard limit of the memory limiter.
  goMemLimitMiB: 340
  # Service Graph settings
  # Service Graph is a feature that allows you to visualize the service graph of your application.
  # It is enabled by default and can be disabled by setting the disabled flag to true.
  serviceGraphDisabled: false


collectorNode:
  # The port to use for exposing the collector's own metrics as a prometheus endpoint.
  # This can be used to resolve conflicting ports when a collector is using the host network.
  collectorOwnMetricsPort: 55682

  # RequestMemoryMiB is the memory request for the node collector daemonset.
  # it will be embedded in the daemonset as a resource request of the form "memory: <value>Mi"
  # default value is 250Mi
  requestMemoryMiB: 250

  # LimitMemoryMiB is the memory limit for the node collector daemonset.
  # it will be embedded in the daemonset as a resource limit of the form "memory: <value>Mi"
  # default value is 2x the memory request.
  limitMemoryMiB: 500

  # the CPU request for the node collector daemonset.
  # it will be embedded in the daemonset as a resource request
  # of the form "cpu: <value>m".
  # default value is 250m
  requestCPUm: 250
  # the CPU limit for the node collector daemonset.
  # it will be embedded in the daemonset as a resource limit
  # of the form "cpu: <value>m".
  # default value is 500m
  limitCPUm: 500

  # this parameter sets the "limit_mib" parameter in the memory limiter configuration for the node collector.
  # it is the hard limit after which a force garbage collection will be performed.
  # if not set, it will be 50Mi below the memory limit.
  memoryLimiterLimitMiB: 450

  # this parameter sets the "spike_limit_mib" parameter in the memory limiter configuration for the node collector.
  # note that this is not the processor soft limit, but the diff in Mib between the hard limit and the soft limit.
  # if not set, this will be set to 20% of the hard limit (so the soft limit will be 80% of the hard limit).
  memoryLimiterSpikeLimitMiB: 55

  # the GOMEMLIMIT environment variable value for the node collector daemonset.
  # this is when go runtime will start garbage collection.
  # if not specified, it will be set to 80% of the hard limit of the memory limiter.
  goMemLimitMiB: 360

  # this configuration is used for logs collection where '/var/log' in a k8s node is a symlink
  # to some other directory (for example, '/mnt/var/log')
  k8sNodeLogsDirectory: ''

autoscaler:
  nodeSelector:
    kubernetes.io/os: linux
  tolerations: []
  affinity: {}
  resources:
    requests:
      cpu: 10m
      memory: 64Mi
    limits:
      cpu: 500m
      memory: 512Mi

scheduler:
  nodeSelector:
    kubernetes.io/os: linux
  tolerations: []
  affinity: {}
  resources:
    requests:
      cpu: 10m
      memory: 64Mi
    limits:
      cpu: 500m
      memory: 512Mi

ui:
  nodeSelector:
    kubernetes.io/os: linux
  tolerations: []
  affinity: {}
  resources:
    requests:
      cpu: 10m
      memory: 64Mi
    limits:
      cpu: 500m
      memory: 512Mi
  # uiMode: 'default' or 'readonly'
  #  - This flag controls whether the UI should be in read-only mode.
  #  - Setting this to "readonly" will disable the ability to create, update, or delete objects in the UI.
  #  - If not set, the UI will be in default mode.
  uiMode: 'default'
  # uiPaginationLimit:
  #  - This flag controls the number of items to fetch per paginated-batch in the UI.
  #  - If not set, the UI will fetch 100 items per paginated-batch.
  uiPaginationLimit: 0
  # uiRemoteUrl:
  #  - This flag sets the URL of the remote UI (e.g. https://my-odigos-ui.com).
  #  - If not set, the UI will default to the local UI.
  #  - This is useful when you are hosting the Odigos UI on a custom/remote URL, and require OIDC authentication.
  uiRemoteUrl: ''
  # oidcTenantUrl:
  #  - This flag sets the URL of the OIDC tenant (e.g. https://my-oidc-tenant.com).
  #  - If not set, the UI will not process OIDC authentication.
  oidcTenantUrl: ''
  # oidcClientId:
  #  - This flag sets the client ID of the OIDC application.
  #  - If not set, the UI will not process OIDC authentication.
  oidcClientId: ''
  # oidcClientSecret:
  #  - This flag sets the client secret of the OIDC application.
  #  - If not set, the UI will not process OIDC authentication.
  oidcClientSecret: ''
  centralBackendURL: ''

instrumentor:
  # which mount method to use for odigos agent directory
  # k8s-virtual-device: default method using a virtual device
  # k8s-host-path: alternative which uses hostPath volume (recommended if supported, requires hostPath volume to be enabled in the cluster)
  mountMethod: ''
  nodeSelector:
    kubernetes.io/os: linux
  tolerations: []
  affinity: {}
  resources:
    requests:
      cpu: 10m
      memory: 64Mi
    limits:
      cpu: 500m
      memory: 512Mi
  # skipWebhookIssuerCreation:
  #   - This flag controls whether the chart should skip creating an Issuer
  #     and associated Certificate for securing the mutating webhook communication.
  #   - Setting this to "true" will skip the creation of an Issuer and Certificate,
  #     even if cert-manager is detected in the cluster and the required API version is available.
  #   - In some cases, the Issuer and Certificate are necessary for the instrumentor service
  #     to run its pod mutating webhook, which requires a signed certificate to comply with
  #     Kubernetes' TLS requirements.
  #   - If cert-manager is not installed or detected, no Issuer or Certificate will be created
  #     regardless of this flag.
  skipWebhookIssuerCreation: false
  # how to add the required environment variables for instrumentation to a container
  # loader: only try using the odigos loader which requires setting the LD_PRELOAD env var in the container manifest.
  # pod-manifest: add the environment variables to the container manifest.
  # loader-fallback-to-pod-manifest: try using the odigos loader first, and if it fails, fallback to adding the environment variables to the container manifest.
  agentEnvVarsInjectionMethod: ''

odiglet:
  nodeSelector:
    kubernetes.io/os: linux
  tolerations:
    ## This toleration with 'Exists' operator and no key/effect specified
    ## will match ALL taints, allowing pods to be scheduled on any node
    ## regardless of its taints (including master/control-plane nodes)
    - operator: Exists
  affinity: {}
  # Resource configuration for the odiglet daemonset
  # resources:
  #   requests:
  #     cpu: 10m
  #     memory: 64Mi
  #   limits:
  #     cpu: 500m
  #     memory: 512Mi
  deviceplugin:
    resources:
      requests:
        cpu: 20m
        memory: 100Mi
      limits:
        cpu: 50m
        memory: 150Mi

  ## Odiglet init container resources, the init container is responsible for copying the instrumentation agents to the host.
  ## There is a tradeoff of using more resources for the init container, and the time it takes to copy the instrumentation agents to the host.
  initContainerResources:
    requests:
      cpu: 200m
      memory: 200Mi
    limits:
      cpu: 200m
      memory: 200Mi

  # in some environments, such as Rancher installations, the container runtime Unix socket is not located in a standard path.
  # In these cases, you should mount the correct socket location (e.g., /var/lib/rancher/rke2/agent/containerd/containerd.sock)
  # into the Odiglet to ensure it can access the container runtime unix socket.
  customContainerRuntimeSocketPath: ''

  # Prior to Kubernetes v1.26, Odigos uses host networking.
  # In some environments, different ports may already be in use on the host (e.g., due to other daemons or networking constraints).
  # Use the following values to configure the readiness and liveness probe ports to avoid conflicts.
  readinessAndLivenessProbePort: 55683

centralProxy:
  enabled: false
  # Central backend URL where this proxy will forward data
  centralBackendURL: ''
  resources:
    requests:
      cpu: 100m
      memory: 64Mi
    limits:
      cpu: 500m
      memory: 256Mi

  nodeSelector:
    kubernetes.io/os: linux
  tolerations: []
  affinity: {}

# Pod Security Policy
psp:
  enabled: false

telemetry:
  enabled: true

openshift:
  enabled: false

gke:
  enabled: false

# List of profile names (array). Example (YAML): profiles: [size_l] | (CLI): --set profiles={size_l}
profiles: []

# Optional NodeSelector to apply to all Odigos components
# Note: Odigos will only be able to instrument workloads on the same nodes.
nodeSelector: {}


# Karpenter settings, before changing to true please modify the karpenter-node-template.yaml file
# to add the odigos-agent startupTaints configuration.
# https://docs.odigos.io/setup/odigos-with-karpenter
karpenter:
  enabled: false


# Refer to the official Kubernetes documentation for structure and field details:
# https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/
#
# Example: Odigos components should be evenly spread across zones.
# No need to set labelSelector manually — the Odigos Helm chart will apply it automatically.
# topologySpreadConstraints:
#   - maxSkew: 1
#     topologyKey: "topology.kubernetes.io/zone"
#     whenUnsatisfiable: "ScheduleAnyway"
topologySpreadConstraints: []


# Auto rollback settings
# The auto‐rollback feature provides a stability window to test instrumented apps.
# After an application is instrumented, a grace period begins during which we allow the app (and its dependencies) to stabilize and start working.
# Once the grace period has passed and we are within the stability window, we can decide to uninstrument a crashing application.
autoRollback:
  disabled: false
  graceTime: 5m
  stabilityWindowTime: 1h

rollout:
# Odigos automatically triggers a one-time rollout for workloads when instrumenting or uninstrumenting, to apply changes.
# If workload restarts are sensitive, this setting can be used to disable the automatic rollout.
# When disabled, users are responsible for manually triggering rollouts after adding or removing sources.
# Any new pods created after enabling or disabling the agent (via manual rollout, autoscaling, etc.)
# will still have the agent injected, regardless of this setting.
# When set to true, all additional configurations related to automated rollouts or rollbacks are ignored.
  automaticRolloutDisabled: false

# Pod Disruption Budgets (PDBs) help ensure high availability during voluntary disruptions like node drains or upgrades.
# When enabled, Odigos will deploy PDBs for its components based on their importance:
# - Critical components (e.g., instrumentor) will be protected with stricter disruption limits.
# - Non-critical components will use more relaxed budgets to allow operational flexibility.
# This reflects Odigos' recommended defaults for maintaining observability continuity without blocking routine maintenance.
# PDBs are disabled by default and can be enabled in production environments where stability during disruptions is important.
pdb:
  enabled: false

---
title: "Pipeline Configuration"
sidebarTitle: "Configurations"
---

Odigos sets up an observability pipeline in your Kubernetes cluster to collect, process and export OpenTelemetry data.

<Frame>
    <img src="/images/odigos_pipeline.png" alt="Odigos Pipeline" />
</Frame>


User can configure the following components:

- **Cluster Gateway Collector** - a collector that runs as a k8s Deployment. It receives the OpenTelemetry data from the Node Collectors, processes it, and exports it to the configured destinations.

### Configuring the Pipeline

Odigos offers two main options for configuring the pipeline:
<Info>
When configuring the pipeline, you must use **only one of the two methods**â€”either profiles or directly editing the `odigos-config` ConfigMap. Using both methods will result in configurations overwriting each other, potentially leading to unexpected behavior.
</Info>

#### 1. Using Profiles

Sizing Profiles `size_s`, `size_m`, `size_l` are pre-defined configurations designed to simplify pipeline configurations. Each profile specifies the following parameters for the **Cluster Gateway Collector**:

| Profile  | Minimum Replicas      | HPA Maximum Replicas      | Request CPU (m)      | Limit CPU (m)      | Request Memory (Mi)
|----------|-----------------------|----------------------|-----------------|---------------|----------------------|
| `size_s` | **1**                 | **5**                | **150m**        | **300m**      | **300Mi**            |
| `size_m` | **2**                 | **8**                | **500m**        | **1000m**     | **500Mi**            |
| `size_l` | **3**                 | **12**               | **750m**        | **1250m**     | **750Mi**            |

To use profiles, you need to use the [Odigos CLI Command for Profiles](/cli/odigos_profile).  
This simplifies the setup process and ensures optimized settings for typical use cases.

#### 2. Directly Editing the `odigos-config` ConfigMap

For more advanced control over the pipeline, you can directly edit the `odigos-config` ConfigMap. This method allows you to configure additional `collectorGateway` settings.

Below is an example of how to configure the `collectorGateway` parameters:

```yaml
collectorGateway:
  # MinReplicas is the number of replicas for the cluster gateway collector deployment.
  # It also sets the minReplicas for the HPA to this value.
  minReplicas: 1

  # MaxReplicas sets the maxReplicas for the HPA to this value.
  maxReplicas: 8

  # RequestMemoryMiB is the memory request for the cluster gateway collector deployment.
  # This value will be embedded in the deployment as a resource request of the form "memory: <value>Mi".
  requestMemoryMiB: 500

  # RequestCPUm is the CPU request for the cluster gateway collector deployment.
  # This value will be embedded in the deployment as a resource request of the form "cpu: <value>m".
  requestCPUm: 500

  # LimitCPUm is the CPU limit for the cluster gateway collector deployment.
  # This value will be embedded in the deployment as a resource limit of the form "cpu: <value>m".
  limitCPUm: 1000

  # MemoryLimiterLimitMiB sets the "limit_mib" parameter in the memory limiter configuration for the collector gateway.
  # This is the hard limit after which a force garbage collection will be performed.
  # Default: 50Mi below the memory request.
  memoryLimiterLimitMiB: 

  # MemoryLimiterSpikeLimitMiB sets the "spike_limit_mib" parameter in the memory limiter configuration.
  # This is the diff in MiB between the hard limit and the soft limit.
  # Default: 20% of the hard limit (soft limit will be 80% of the hard limit).
  memoryLimiterSpikeLimitMiB: 

  # GoMemLimitMib sets the GOMEMLIMIT environment variable value for the collector gateway deployment.
  # This is when the Go runtime will start garbage collection.
  # Default: 80% of the hard limit of the memory limiter.
  goMemLimitMiB:
```


Editing the ConfigMap allows you to fine-tune every aspect of the pipeline to meet custom requirements.  
If you need any help fine-tuning the configurations, you can reach out to the Odigos team for support.
package graph

// This file will be automatically regenerated based on the schema, any resolver implementations
// will be copied through when generating and any unknown code will be moved to the end.
// Code generated by github.com/99designs/gqlgen version v0.17.70

import (
	"context"
	"fmt"
	"time"

	"github.com/99designs/gqlgen/graphql"
	"github.com/odigos-io/odigos/api/k8sconsts"
	"github.com/odigos-io/odigos/frontend/graph/loaders"
	"github.com/odigos-io/odigos/frontend/graph/model"
	"github.com/odigos-io/odigos/frontend/graph/status"
	"github.com/odigos-io/odigos/frontend/services/common"
	sourceutils "github.com/odigos-io/odigos/k8sutils/pkg/source"
)

// WorkloadOdigosHealthStatus is the resolver for the workloadOdigosHealthStatus field.
func (r *k8sWorkloadResolver) WorkloadOdigosHealthStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	conditions := []*model.DesiredConditionStatus{}
	if ic != nil {
		conditions = append(conditions, status.CalculateRuntimeInspectionStatus(ic))
		conditions = append(conditions, status.CalculateAgentInjectionEnabledStatus(ic))
		conditions = append(conditions, status.CalculateRolloutStatus(ic))
	} else {
		reasonStr := string(status.WorkloadOdigosHealthStatusReasonDisabled)
		conditions = append(conditions, &model.DesiredConditionStatus{
			Name:       status.WorkloadOdigosHealthStatus,
			Status:     model.DesiredStateProgressDisabled,
			ReasonEnum: &reasonStr,
			Message:    "workload is not marked for instrumentation",
		})
	}

	// always report if agent is injected or not, even if the workload is not marked for instrumentation.
	// this is to detect if uninstrumented pods have agent injected when it should not.
	conditions = append(conditions, status.CalculateAgentInjectedStatus(ic, pods))

	mostSevereState := &model.DesiredConditionStatus{
		Name:       status.WorkloadOdigosHealthStatus,
		Status:     model.DesiredStateProgressUnknown,
		ReasonEnum: nil,
		Message:    "",
	}
	mostSevereSeverity := desiredStateProgressSeverity(mostSevereState.Status)
	for _, condition := range conditions {
		if condition == nil {
			continue
		}
		severity := desiredStateProgressSeverity(condition.Status)
		if severity < mostSevereSeverity {
			mostSevereSeverity = severity
			mostSevereState = condition
		}
	}

	// exception, if all is well, we return a special condition to denote it
	if mostSevereState.Status == model.DesiredStateProgressSuccess {

		workloadMetrics, ok := r.MetricsConsumer.GetSingleSourceMetrics(common.SourceID{
			Namespace: obj.ID.Namespace,
			Kind:      k8sconsts.WorkloadKind(obj.ID.Kind),
			Name:      obj.ID.Name,
		})
		var totalDataSent *int
		if ok {
			tds := int(workloadMetrics.TotalDataSent())
			totalDataSent = &tds
		}

		// consider the telemetry metrics status if relevant.
		telemetryMetrics := status.CalculateExpectingTelemetryStatus(ic, pods, totalDataSent)
		expectingTelemetry := telemetryMetrics != nil && telemetryMetrics.IsExpectingTelemetry != nil && *telemetryMetrics.IsExpectingTelemetry

		var reasonStr, message string
		if expectingTelemetry {
			if telemetryMetrics.TelemetryObservedStatus.Status == model.DesiredStateProgressSuccess {
				reasonStr = string(status.WorkloadOdigosHealthStatusReasonSuccessAndEmittingTelemetry)
				message = "source is instrumented, healthy and telemetry has been observed"
			} else {
				reasonStr = string(status.WorkloadOdigosHealthStatusReasonSuccess)
				message = "source is instrumented and healthy, no telemetry recorded yet"
			}
		} else {
			reasonStr = string(status.WorkloadOdigosHealthStatusReasonSuccess)
			message = "source is healthy, no telemetry is expected"
		}
		return &model.DesiredConditionStatus{
			Name:       status.WorkloadOdigosHealthStatus,
			Status:     model.DesiredStateProgressSuccess,
			ReasonEnum: &reasonStr,
			Message:    message,
		}, nil
	}

	return mostSevereState, nil
}

// MarkedForInstrumentation is the resolver for the markedForInstrumentation field.
func (r *k8sWorkloadResolver) MarkedForInstrumentation(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadMakredForInstrumentation, error) {
	l := loaders.For(ctx)
	sources, err := l.GetSources(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	enabled, reason, err := sourceutils.IsObjectInstrumentedBySource(ctx, sources, err)
	if err != nil {
		return nil, err
	}

	return &model.K8sWorkloadMakredForInstrumentation{
		MarkedForInstrumentation: enabled,
		DecisionEnum:             string(reason.Reason),
		Message:                  reason.Message,
	}, nil
}

// RuntimeInfo is the resolver for the runtimeInfo field.
func (r *k8sWorkloadResolver) RuntimeInfo(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadRuntimeInfo, error) {
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	containers := make([]*model.K8sWorkloadRuntimeInfoContainer, len(ic.Status.RuntimeDetailsByContainer))
	for i, container := range ic.Status.RuntimeDetailsByContainer {
		containers[i] = runtimeDetailsContainersToModel(&container)
	}

	runtimeInfo := &model.K8sWorkloadRuntimeInfo{
		Completed:       len(ic.Status.RuntimeDetailsByContainer) > 0,
		CompletedStatus: status.CalculateRuntimeInspectionStatus(ic),
		Containers:      containers,
	}

	return runtimeInfo, nil
}

// AgentEnabled is the resolver for the agentEnabled field.
func (r *k8sWorkloadResolver) AgentEnabled(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadAgentEnabled, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	containers := make([]*model.K8sWorkloadAgentEnabledContainer, 0, len(ic.Spec.Containers))
	for _, container := range ic.Spec.Containers {
		containerModel := agentEnabledContainersToModel(&container)
		containers = append(containers, containerModel)
	}

	return &model.K8sWorkloadAgentEnabled{
		AgentEnabled:  ic.Spec.AgentInjectionEnabled,
		EnabledStatus: status.CalculateAgentInjectionEnabledStatus(ic),
		Containers:    containers,
	}, nil
}

// Rollout is the resolver for the rollout field.
func (r *k8sWorkloadResolver) Rollout(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadRollout, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	rolloutStatus := status.CalculateRolloutStatus(ic)
	if rolloutStatus == nil {
		return nil, nil
	}

	return &model.K8sWorkloadRollout{
		RolloutStatus: rolloutStatus,
	}, nil
}

// Containers is the resolver for the containers field.
func (r *k8sWorkloadResolver) Containers(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadContainer, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	containerByName := make(map[string]*model.K8sWorkloadContainer)
	for _, container := range ic.Spec.Containers {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		containerByName[container.ContainerName].AgentEnabled = agentEnabledContainersToModel(&container)
	}

	for _, container := range ic.Status.RuntimeDetailsByContainer {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		containerByName[container.ContainerName].RuntimeInfo = runtimeDetailsContainersToModel(&container)
	}

	for _, container := range ic.Spec.ContainersOverrides {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		overrides := &model.K8sWorkloadContainerOverrides{
			ContainerName: container.ContainerName,
		}
		if container.RuntimeInfo != nil {
			overrides.RuntimeInfo = runtimeDetailsContainersToModel(container.RuntimeInfo)
		}
		containerByName[container.ContainerName].Overrides = overrides
	}

	containers := make([]*model.K8sWorkloadContainer, 0, len(containerByName))
	for _, container := range containerByName {
		containers = append(containers, container)
	}

	return containers, nil
}

// Pods is the resolver for the pods field.
func (r *k8sWorkloadResolver) Pods(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadPod, error) {
	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	podModels := make([]*model.K8sWorkloadPod, 0, len(pods))
	for _, cachePod := range pods {
		containerModels := make([]*model.K8sWorkloadPodContainer, 0, len(cachePod.Pod.Spec.Containers))

		// set aggregated pod health status to success and override if any container is not healthy.
		podHealthReasonStr := string(PodContainerHealthReasonHealthy)
		podHealthStatus := &model.DesiredConditionStatus{
			Name:       podHealthStatus,
			Status:     model.DesiredStateProgressSuccess,
			ReasonEnum: &podHealthReasonStr,
			Message:    "all containers are healthy",
		}
		for _, container := range cachePod.Pod.Spec.Containers {
			containerStatus := getContainerStatus(cachePod.Pod, container.Name)
			var started, ready *bool
			var isCrashLoop bool = false // never set to nil
			var runninsStartedTime, waitingReasonEnum, waitingMessage *string
			var restartCount *int
			if containerStatus != nil {
				started = containerStatus.Started
				ready = &containerStatus.Ready
				restartCountInt := int(containerStatus.RestartCount)
				restartCount = &restartCountInt
				if containerStatus.State.Waiting != nil {
					isCrashLoop = containerStatus.State.Waiting.Reason == "CrashLoopBackOff"
					waitingReasonEnum = &containerStatus.State.Waiting.Reason
					waitingMessage = &containerStatus.State.Waiting.Message
				}
				if containerStatus.State.Running != nil {
					runningStartedTimeStr := containerStatus.State.Running.StartedAt.Format(time.RFC3339)
					runninsStartedTime = &runningStartedTimeStr
				}
			}

			// pod container is considered healthy if it is started, ready and not in crash loop back off.
			healthStatus := &model.DesiredConditionStatus{
				Name: podContainerHealthStatus,
			}
			if isCrashLoop {
				reasonStr := string(PodContainerHealthReasonCrashLoopBackOff)

				healthStatus.Status = model.DesiredStateProgressError
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod in crash loop back off: " + *waitingReasonEnum

				podHealthStatus.Status = model.DesiredStateProgressError
				podHealthStatus.ReasonEnum = &reasonStr
				podHealthStatus.Message = "container in pod is in crash loop back off"

			} else if started == nil || !*started {
				reasonStr := string(PodContainerHealthReasonNotStarted)

				healthStatus.Status = model.DesiredStateProgressWaiting
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod not started yet"

				if podHealthStatus.Status != model.DesiredStateProgressError {
					podHealthStatus.Status = model.DesiredStateProgressWaiting
					podHealthStatus.ReasonEnum = &reasonStr
					podHealthStatus.Message = "container in pod is not started yet"
				}

			} else if ready == nil || !*ready {
				reasonStr := string(PodContainerHealthReasonNotReady)

				healthStatus.Status = model.DesiredStateProgressWaiting
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod not ready yet"

				if podHealthStatus.Status != model.DesiredStateProgressError {
					podHealthStatus.Status = model.DesiredStateProgressWaiting
					podHealthStatus.ReasonEnum = &reasonStr
					podHealthStatus.Message = "container in pod is not ready yet"
				}
			} else {
				reasonStr := string(PodContainerHealthReasonHealthy)
				healthStatus.Status = model.DesiredStateProgressSuccess
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod is healthy"
			}

			containerModels = append(containerModels, &model.K8sWorkloadPodContainer{
				ContainerName:      container.Name,
				Started:            started,
				Ready:              ready,
				IsCrashLoop:        &isCrashLoop,
				RestartCount:       restartCount,
				RunningStartedTime: runninsStartedTime,
				WaitingReasonEnum:  waitingReasonEnum,
				WaitingMessage:     waitingMessage,
				HealthStatus:       healthStatus,
			})
		}
		podModels = append(podModels, &model.K8sWorkloadPod{
			PodName:             cachePod.Pod.Name,
			NodeName:            cachePod.Pod.Spec.NodeName,
			StartTime:           cachePod.Pod.CreationTimestamp.Format(time.RFC3339),
			AgentInjected:       cachePod.ComputedPodValues.AgentInjected,
			AgentInjectedStatus: cachePod.ComputedPodValues.AgentInjectedStatus,
			// TODO: RunningLatestWorkloadRevision
			Containers:      containerModels,
			PodHealthStatus: podHealthStatus,
		})
	}
	return podModels, nil
}

// PodsAgentInjectionStatus is the resolver for the podsAgentInjectionStatus field.
func (r *k8sWorkloadResolver) PodsAgentInjectionStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}
	instrumentationConfig, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	agentInjectionStatus := status.CalculateAgentInjectedStatus(instrumentationConfig, pods)
	return agentInjectionStatus, nil
}

// TelemetryMetrics is the resolver for the telemetryMetrics field.
func (r *k8sWorkloadResolver) TelemetryMetrics(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadTelemetryMetrics, error) {
	var totalDataSent *int
	var throughput *int

	// attempt to get the metrics for the workload, or keep them as nil if not available.
	workloadMetrics, ok := r.MetricsConsumer.GetSingleSourceMetrics(common.SourceID{
		Namespace: obj.ID.Namespace,
		Kind:      k8sconsts.WorkloadKind(obj.ID.Kind),
		Name:      obj.ID.Name,
	})
	if ok {
		tds := int(workloadMetrics.TotalDataSent())
		tp := int(workloadMetrics.TotalThroughput())
		totalDataSent = &tds
		throughput = &tp
	}

	return []*model.K8sWorkloadTelemetryMetrics{
		{
			TotalDataSentBytes: totalDataSent,
			ThroughputBytes:    throughput,
		},
	}, nil
}

// ExpectingTelemetry is the resolver for the expectingTelemetry field.
func (r *k8sWorkloadTelemetryMetricsResolver) ExpectingTelemetry(ctx context.Context, obj *model.K8sWorkloadTelemetryMetrics) (*model.K8sWorkloadTelemetryMetricsExpectingTelemetryStatus, error) {
	// Safely derive the parent workload ID from the GraphQL field context
	var workloadId model.K8sWorkloadID
	fc := graphql.GetFieldContext(ctx)
	// go up 3 levels in the context to get the workload id
	// current field -> *model.K8sWorkloadTelemetryMetrics -> []*model.K8sWorkloadTelemetryMetrics -> *model.K8sWorkload
	if fc == nil || fc.Parent == nil || fc.Parent.Parent == nil || fc.Parent.Parent.Parent == nil {
		return nil, fmt.Errorf("missisng parent resolver context")
	}
	w, ok := fc.Parent.Parent.Parent.Result.(**model.K8sWorkload)
	if !ok || w == nil || (*w).ID == nil {
		return nil, fmt.Errorf("parent is not a workload")
	}
	workloadId = *(*w).ID

	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, workloadId)
	if err != nil {
		return nil, err
	}
	ic, err := l.GetInstrumentationConfig(ctx, workloadId)
	if err != nil {
		return nil, err
	}

	return status.CalculateExpectingTelemetryStatus(ic, pods, obj.TotalDataSentBytes), nil
}

// K8sWorkload returns K8sWorkloadResolver implementation.
func (r *Resolver) K8sWorkload() K8sWorkloadResolver { return &k8sWorkloadResolver{r} }

// K8sWorkloadTelemetryMetrics returns K8sWorkloadTelemetryMetricsResolver implementation.
func (r *Resolver) K8sWorkloadTelemetryMetrics() K8sWorkloadTelemetryMetricsResolver {
	return &k8sWorkloadTelemetryMetricsResolver{r}
}

type k8sWorkloadResolver struct{ *Resolver }
type k8sWorkloadTelemetryMetricsResolver struct{ *Resolver }

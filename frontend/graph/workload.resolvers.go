package graph

// This file will be automatically regenerated based on the schema, any resolver implementations
// will be copied through when generating and any unknown code will be moved to the end.
// Code generated by github.com/99designs/gqlgen version v0.17.70

import (
	"context"
	"fmt"

	"github.com/99designs/gqlgen/graphql"
	"github.com/odigos-io/odigos/api/k8sconsts"
	odigosv1 "github.com/odigos-io/odigos/api/odigos/v1alpha1"
	"github.com/odigos-io/odigos/frontend/graph/loaders"
	"github.com/odigos-io/odigos/frontend/graph/model"
	"github.com/odigos-io/odigos/frontend/graph/status"
	"github.com/odigos-io/odigos/frontend/services/common"
	sourceutils "github.com/odigos-io/odigos/k8sutils/pkg/source"
)

// WorkloadOdigosHealthStatus is the resolver for the workloadOdigosHealthStatus field.
func (r *k8sWorkloadResolver) WorkloadOdigosHealthStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	conditions := []*model.DesiredConditionStatus{}
	if ic != nil {
		conditions = append(conditions, status.CalculateRuntimeInspectionStatus(ic))
		conditions = append(conditions, status.CalculateAgentInjectionEnabledStatus(ic))
		conditions = append(conditions, status.CalculateRolloutStatus(ic))
	} else {
		reasonStr := string(status.WorkloadOdigosHealthStatusReasonDisabled)
		conditions = append(conditions, &model.DesiredConditionStatus{
			Name:       status.WorkloadOdigosHealthStatus,
			Status:     model.DesiredStateProgressDisabled,
			ReasonEnum: &reasonStr,
			Message:    "workload is not marked for instrumentation",
		})
	}

	// always report if agent is injected or not, even if the workload is not marked for instrumentation.
	// this is to detect if uninstrumented pods have agent injected when it should not.
	conditions = append(conditions, status.CalculateAgentInjectedStatus(ic, pods))
	aggregateContainerProcessesHealth, err := aggregateProcessesHealthForWorkload(ctx, obj.ID)
	if err != nil {
		return nil, err
	}
	conditions = append(conditions, aggregateContainerProcessesHealth)

	mostSevereState := &model.DesiredConditionStatus{
		Name:       status.WorkloadOdigosHealthStatus,
		Status:     model.DesiredStateProgressUnknown,
		ReasonEnum: nil,
		Message:    "",
	}
	mostSevereSeverity := desiredStateProgressSeverity(mostSevereState.Status)
	for _, condition := range conditions {
		if condition == nil {
			continue
		}
		severity := desiredStateProgressSeverity(condition.Status)
		if severity < mostSevereSeverity {
			mostSevereSeverity = severity
			mostSevereState = condition
		}
	}

	// exception, if all is well, we return a special condition to denote it
	if mostSevereState.Status == model.DesiredStateProgressSuccess {

		workloadMetrics, ok := r.MetricsConsumer.GetSingleSourceMetrics(common.SourceID{
			Namespace: obj.ID.Namespace,
			Kind:      k8sconsts.WorkloadKind(obj.ID.Kind),
			Name:      obj.ID.Name,
		})
		var totalDataSent *int
		if ok {
			tds := int(workloadMetrics.TotalDataSent())
			totalDataSent = &tds
		}

		// consider the telemetry metrics status if relevant.
		telemetryMetrics := status.CalculateExpectingTelemetryStatus(ic, pods, totalDataSent)
		expectingTelemetry := telemetryMetrics != nil && telemetryMetrics.IsExpectingTelemetry != nil && *telemetryMetrics.IsExpectingTelemetry

		var reasonStr, message string
		if expectingTelemetry {
			if telemetryMetrics.TelemetryObservedStatus.Status == model.DesiredStateProgressSuccess {
				reasonStr = string(status.WorkloadOdigosHealthStatusReasonSuccessAndEmittingTelemetry)
				message = "source is instrumented, healthy and telemetry has been observed"
			} else {
				reasonStr = string(status.WorkloadOdigosHealthStatusReasonSuccess)
				message = "source is instrumented and healthy, no telemetry recorded yet"
			}
		} else {
			reasonStr = string(status.WorkloadOdigosHealthStatusReasonSuccess)
			message = "source is healthy, no telemetry is expected"
		}
		return &model.DesiredConditionStatus{
			Name:       status.WorkloadOdigosHealthStatus,
			Status:     model.DesiredStateProgressSuccess,
			ReasonEnum: &reasonStr,
			Message:    message,
		}, nil
	}

	return mostSevereState, nil
}

// MarkedForInstrumentation is the resolver for the markedForInstrumentation field.
func (r *k8sWorkloadResolver) MarkedForInstrumentation(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadMarkedForInstrumentation, error) {
	l := loaders.For(ctx)
	sources, err := l.GetSources(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	enabled, reason, err := sourceutils.IsObjectInstrumentedBySource(ctx, sources, err)
	if err != nil {
		return nil, err
	}

	var markedForInstrumentation *bool
	if enabled {
		markedForInstrumentation = &enabled
	} else {
		if reason.Reason == string(odigosv1.MarkedForInstrumentationReasonWorkloadSourceDisabled) {
			markedForInstrumentation = &enabled
		}
	}

	return &model.K8sWorkloadMarkedForInstrumentation{
		MarkedForInstrumentation: markedForInstrumentation,
		DecisionEnum:             string(reason.Reason),
		Message:                  reason.Message,
	}, nil
}

// RuntimeInfo is the resolver for the runtimeInfo field.
func (r *k8sWorkloadResolver) RuntimeInfo(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadRuntimeInfo, error) {
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	containers := make([]*model.K8sWorkloadRuntimeInfoContainer, len(ic.Status.RuntimeDetailsByContainer))
	for i, container := range ic.Status.RuntimeDetailsByContainer {
		containers[i] = runtimeDetailsContainersToModel(&container)
	}

	runtimeInfo := &model.K8sWorkloadRuntimeInfo{
		Completed:       len(ic.Status.RuntimeDetailsByContainer) > 0,
		CompletedStatus: status.CalculateRuntimeInspectionStatus(ic),
		Containers:      containers,
	}

	return runtimeInfo, nil
}

// AgentEnabled is the resolver for the agentEnabled field.
func (r *k8sWorkloadResolver) AgentEnabled(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadAgentEnabled, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	containers := make([]*model.K8sWorkloadAgentEnabledContainer, 0, len(ic.Spec.Containers))
	for _, container := range ic.Spec.Containers {
		containerModel := agentEnabledContainersToModel(&container)
		containers = append(containers, containerModel)
	}

	return &model.K8sWorkloadAgentEnabled{
		AgentEnabled:  ic.Spec.AgentInjectionEnabled,
		EnabledStatus: status.CalculateAgentInjectionEnabledStatus(ic),
		Containers:    containers,
	}, nil
}

// Rollout is the resolver for the rollout field.
func (r *k8sWorkloadResolver) Rollout(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadRollout, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	rolloutStatus := status.CalculateRolloutStatus(ic)
	if rolloutStatus == nil {
		return nil, nil
	}

	return &model.K8sWorkloadRollout{
		RolloutStatus: rolloutStatus,
	}, nil
}

// Containers is the resolver for the containers field.
func (r *k8sWorkloadResolver) Containers(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadContainer, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	containerByName := make(map[string]*model.K8sWorkloadContainer)
	for _, container := range ic.Spec.Containers {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		containerByName[container.ContainerName].AgentEnabled = agentEnabledContainersToModel(&container)
	}

	for _, container := range ic.Status.RuntimeDetailsByContainer {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		containerByName[container.ContainerName].RuntimeInfo = runtimeDetailsContainersToModel(&container)
	}

	for _, container := range ic.Spec.ContainersOverrides {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		overrides := &model.K8sWorkloadContainerOverrides{
			ContainerName: container.ContainerName,
		}
		if container.RuntimeInfo != nil {
			overrides.RuntimeInfo = runtimeDetailsContainersToModel(container.RuntimeInfo)
		}
		containerByName[container.ContainerName].Overrides = overrides
	}

	containers := make([]*model.K8sWorkloadContainer, 0, len(containerByName))
	for _, container := range containerByName {
		containers = append(containers, container)
	}

	return containers, nil
}

// Pods is the resolver for the pods field.
func (r *k8sWorkloadResolver) Pods(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadPod, error) {
	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	podModels := make([]*model.K8sWorkloadPod, 0, len(pods))
	for _, pod := range pods {
		containerModels := make([]*model.K8sWorkloadPodContainer, 0, len(pod.Containers))

		// set aggregated pod health status to success and override if any container is not healthy.
		podHealthReasonStr := string(PodContainerHealthReasonHealthy)
		podHealthStatus := &model.DesiredConditionStatus{
			Name:       podHealthStatus,
			Status:     model.DesiredStateProgressSuccess,
			ReasonEnum: &podHealthReasonStr,
			Message:    "all containers are healthy",
		}
		for _, container := range pod.Containers {

			// pod container is considered healthy if it is started, ready and not in crash loop back off.
			healthStatus := &model.DesiredConditionStatus{
				Name: podContainerHealthStatus,
			}
			if container.IsCrashLoop {
				reasonStr := string(PodContainerHealthReasonCrashLoopBackOff)

				healthStatus.Status = model.DesiredStateProgressError
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod in crash loop back off: " + *container.WaitingReasonEnum

				podHealthStatus.Status = model.DesiredStateProgressError
				podHealthStatus.ReasonEnum = &reasonStr
				podHealthStatus.Message = "container in pod is in crash loop back off"

			} else if container.Started == nil || !*container.Started {
				reasonStr := string(PodContainerHealthReasonNotStarted)

				healthStatus.Status = model.DesiredStateProgressWaiting
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod not started yet"

				if podHealthStatus.Status != model.DesiredStateProgressError {
					podHealthStatus.Status = model.DesiredStateProgressWaiting
					podHealthStatus.ReasonEnum = &reasonStr
					podHealthStatus.Message = "container in pod is not started yet"
				}

			} else if !container.IsReady {
				reasonStr := string(PodContainerHealthReasonNotReady)

				healthStatus.Status = model.DesiredStateProgressWaiting
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod not ready yet"

				if podHealthStatus.Status != model.DesiredStateProgressError {
					podHealthStatus.Status = model.DesiredStateProgressWaiting
					podHealthStatus.ReasonEnum = &reasonStr
					podHealthStatus.Message = "container in pod is not ready yet"
				}
			} else {
				reasonStr := string(PodContainerHealthReasonHealthy)
				healthStatus.Status = model.DesiredStateProgressSuccess
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod is healthy"
			}

			containerModels = append(containerModels, &model.K8sWorkloadPodContainer{
				ContainerName:                   container.ContainerName,
				OtelDistroName:                  container.OtelDistroName,
				OdigosInstrumentationDeviceName: container.OdigosInstrumentationDeviceName,
				Started:                         container.Started,
				Ready:                           container.Ready,
				IsCrashLoop:                     &container.IsCrashLoop,
				RestartCount:                    container.RestartCount,
				RunningStartedTime:              container.RunningStartedTime,
				WaitingReasonEnum:               container.WaitingReasonEnum,
				WaitingMessage:                  container.WaitingMessage,
				HealthStatus:                    healthStatus,
			})
		}
		podModels = append(podModels, &model.K8sWorkloadPod{
			PodName:             pod.PodName,
			NodeName:            pod.PodNodeName,
			StartTime:           pod.PodStartTime,
			AgentInjected:       pod.AgentInjected,
			AgentInjectedStatus: pod.AgentInjectedStatus,
			// TODO: RunningLatestWorkloadRevision
			Containers:      containerModels,
			PodHealthStatus: podHealthStatus,
		})
	}
	return podModels, nil
}

// PodsAgentInjectionStatus is the resolver for the podsAgentInjectionStatus field.
func (r *k8sWorkloadResolver) PodsAgentInjectionStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}
	instrumentationConfig, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	agentInjectionStatus := status.CalculateAgentInjectedStatus(instrumentationConfig, pods)
	return agentInjectionStatus, nil
}

// ProcessesHealthStatus is the resolver for the processesHealthStatus field.
func (r *k8sWorkloadResolver) ProcessesHealthStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	return aggregateProcessesHealthForWorkload(ctx, obj.ID)
}

// TelemetryMetrics is the resolver for the telemetryMetrics field.
func (r *k8sWorkloadResolver) TelemetryMetrics(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadTelemetryMetrics, error) {
	var totalDataSent *int
	var throughput *int

	// attempt to get the metrics for the workload, or keep them as nil if not available.
	workloadMetrics, ok := r.MetricsConsumer.GetSingleSourceMetrics(common.SourceID{
		Namespace: obj.ID.Namespace,
		Kind:      k8sconsts.WorkloadKind(obj.ID.Kind),
		Name:      obj.ID.Name,
	})
	if ok {
		tds := int(workloadMetrics.TotalDataSent())
		tp := int(workloadMetrics.TotalThroughput())
		totalDataSent = &tds
		throughput = &tp
	}

	return []*model.K8sWorkloadTelemetryMetrics{
		{
			TotalDataSentBytes: totalDataSent,
			ThroughputBytes:    throughput,
		},
	}, nil
}

// Processes is the resolver for the processes field.
func (r *k8sWorkloadPodContainerResolver) Processes(ctx context.Context, obj *model.K8sWorkloadPodContainer) ([]*model.K8sWorkloadPodContainerProcess, error) {
	l := loaders.For(ctx)

	containerName := obj.ContainerName

	// extract pod info from the parent resolver context
	fc := graphql.GetFieldContext(ctx)
	if fc == nil || fc.Parent == nil || fc.Parent.Parent == nil || fc.Parent.Parent.Parent == nil {
		return nil, fmt.Errorf("missisng parent resolver context")
	}
	workloadPodCtx := fc.Parent.Parent.Parent
	p, ok := workloadPodCtx.Result.(**model.K8sWorkloadPod)
	if !ok || p == nil || (*p).PodName == "" {
		return nil, fmt.Errorf("parent is not a pod")
	}
	podName := (*p).PodName

	if workloadPodCtx.Parent == nil || workloadPodCtx.Parent.Parent == nil {
		return nil, fmt.Errorf("missing parent resolver context")
	}
	workloadCtx := workloadPodCtx.Parent.Parent
	c, ok := workloadCtx.Result.(**model.K8sWorkload)
	if !ok || c == nil || (*c).ID == nil {
		return nil, fmt.Errorf("parent is not a workload")
	}
	workloadId := *(*c).ID

	instrumentationInstances, err := l.GetInstrumentationInstancesForContainer(ctx, loaders.ContainerId{
		Namespace:     workloadId.Namespace,
		PodName:       podName,
		ContainerName: containerName,
	})
	if err != nil {
		return nil, err
	}

	processes := make([]*model.K8sWorkloadPodContainerProcess, 0, len(instrumentationInstances))
	for _, instrumentationInstance := range instrumentationInstances {
		processHealthStatus := status.CalculateProcessHealthStatus(instrumentationInstance)
		identifyingAttributes := make([]*model.K8sWorkloadPodContainerProcessAttribute, 0, len(instrumentationInstance.Status.IdentifyingAttributes))
		for _, attribute := range instrumentationInstance.Status.IdentifyingAttributes {
			identifyingAttributes = append(identifyingAttributes, &model.K8sWorkloadPodContainerProcessAttribute{
				Name:  attribute.Key,
				Value: attribute.Value,
			})
		}
		processes = append(processes, &model.K8sWorkloadPodContainerProcess{
			Healthy:               instrumentationInstance.Status.Healthy,
			HealthStatus:          processHealthStatus,
			IdentifyingAttributes: identifyingAttributes,
		})
	}
	return processes, nil
}

// ExpectingTelemetry is the resolver for the expectingTelemetry field.
func (r *k8sWorkloadTelemetryMetricsResolver) ExpectingTelemetry(ctx context.Context, obj *model.K8sWorkloadTelemetryMetrics) (*model.K8sWorkloadTelemetryMetricsExpectingTelemetryStatus, error) {
	// Safely derive the parent workload ID from the GraphQL field context
	var workloadId model.K8sWorkloadID
	fc := graphql.GetFieldContext(ctx)
	// go up 3 levels in the context to get the workload id
	// current field -> *model.K8sWorkloadTelemetryMetrics -> []*model.K8sWorkloadTelemetryMetrics -> *model.K8sWorkload
	if fc == nil || fc.Parent == nil || fc.Parent.Parent == nil || fc.Parent.Parent.Parent == nil {
		return nil, fmt.Errorf("missisng parent resolver context")
	}
	w, ok := fc.Parent.Parent.Parent.Result.(**model.K8sWorkload)
	if !ok || w == nil || (*w).ID == nil {
		return nil, fmt.Errorf("parent is not a workload")
	}
	workloadId = *(*w).ID

	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, workloadId)
	if err != nil {
		return nil, err
	}
	ic, err := l.GetInstrumentationConfig(ctx, workloadId)
	if err != nil {
		return nil, err
	}

	return status.CalculateExpectingTelemetryStatus(ic, pods, obj.TotalDataSentBytes), nil
}

// K8sWorkload returns K8sWorkloadResolver implementation.
func (r *Resolver) K8sWorkload() K8sWorkloadResolver { return &k8sWorkloadResolver{r} }

// K8sWorkloadPodContainer returns K8sWorkloadPodContainerResolver implementation.
func (r *Resolver) K8sWorkloadPodContainer() K8sWorkloadPodContainerResolver {
	return &k8sWorkloadPodContainerResolver{r}
}

// K8sWorkloadTelemetryMetrics returns K8sWorkloadTelemetryMetricsResolver implementation.
func (r *Resolver) K8sWorkloadTelemetryMetrics() K8sWorkloadTelemetryMetricsResolver {
	return &k8sWorkloadTelemetryMetricsResolver{r}
}

type k8sWorkloadResolver struct{ *Resolver }
type k8sWorkloadPodContainerResolver struct{ *Resolver }
type k8sWorkloadTelemetryMetricsResolver struct{ *Resolver }

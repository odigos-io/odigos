package graph

// This file will be automatically regenerated based on the schema, any resolver implementations
// will be copied through when generating and any unknown code will be moved to the end.
// Code generated by github.com/99designs/gqlgen version v0.17.70

import (
	"context"
	"fmt"

	"github.com/99designs/gqlgen/graphql"
	"github.com/odigos-io/odigos/api/k8sconsts"
	odigosv1 "github.com/odigos-io/odigos/api/odigos/v1alpha1"
	"github.com/odigos-io/odigos/common"
	"github.com/odigos-io/odigos/frontend/graph/loaders"
	"github.com/odigos-io/odigos/frontend/graph/model"
	"github.com/odigos-io/odigos/frontend/graph/status"
	frontendcommon "github.com/odigos-io/odigos/frontend/services/common"
	sourceutils "github.com/odigos-io/odigos/k8sutils/pkg/source"
)

// ServiceName is the resolver for the serviceName field.
func (r *k8sWorkloadResolver) ServiceName(ctx context.Context, obj *model.K8sWorkload) (*string, error) {
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}
	if ic == nil {
		return nil, nil
	}
	return &ic.Spec.ServiceName, nil
}

// WorkloadOdigosHealthStatus is the resolver for the workloadOdigosHealthStatus field.
func (r *k8sWorkloadResolver) WorkloadOdigosHealthStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	conditions := []*model.DesiredConditionStatus{}
	if ic != nil {
		conditions = append(conditions, status.CalculateRuntimeInspectionStatus(ic))
		conditions = append(conditions, status.CalculateAgentInjectionEnabledStatus(ic))
		conditions = append(conditions, status.CalculateRolloutStatus(ic))
	} else {
		reasonStr := string(status.WorkloadOdigosHealthStatusReasonDisabled)
		conditions = append(conditions, &model.DesiredConditionStatus{
			Name:       status.WorkloadOdigosHealthStatus,
			Status:     model.DesiredStateProgressDisabled,
			ReasonEnum: &reasonStr,
			Message:    "workload is not marked for instrumentation",
		})
	}

	// always report if agent is injected or not, even if the workload is not marked for instrumentation.
	// this is to detect if uninstrumented pods have agent injected when it should not.
	conditions = append(conditions, status.CalculateAgentInjectedStatus(ic, pods))
	aggregateContainerProcessesHealth, err := aggregateProcessesHealthForWorkload(ctx, obj.ID)
	if err != nil {
		return nil, err
	}
	conditions = append(conditions, aggregateContainerProcessesHealth)

	mostSevereCondition := aggregateConditionsBySeverity(conditions)
	if mostSevereCondition == nil {
		mostSevereCondition = &model.DesiredConditionStatus{
			Name:       status.WorkloadOdigosHealthStatus,
			Status:     model.DesiredStateProgressUnknown,
			ReasonEnum: nil,
			Message:    "",
		}
	}

	// exception, if all is well, we return a special condition to denote it
	if mostSevereCondition.Status == model.DesiredStateProgressSuccess {

		workloadMetrics, ok := r.MetricsConsumer.GetSingleSourceMetrics(frontendcommon.SourceID{
			Namespace: obj.ID.Namespace,
			Kind:      k8sconsts.WorkloadKind(obj.ID.Kind),
			Name:      obj.ID.Name,
		})
		var totalDataSent *int
		if ok {
			tds := int(workloadMetrics.TotalDataSent())
			totalDataSent = &tds
		}

		// consider the telemetry metrics status if relevant.
		telemetryMetrics := status.CalculateExpectingTelemetryStatus(ic, pods, totalDataSent)
		expectingTelemetry := telemetryMetrics != nil && telemetryMetrics.IsExpectingTelemetry != nil && *telemetryMetrics.IsExpectingTelemetry

		var reasonStr, message string
		if expectingTelemetry {
			if telemetryMetrics.TelemetryObservedStatus.Status == model.DesiredStateProgressSuccess {
				reasonStr = string(status.WorkloadOdigosHealthStatusReasonSuccessAndEmittingTelemetry)
				message = "source is instrumented, healthy and telemetry has been observed"
			} else {
				reasonStr = string(status.WorkloadOdigosHealthStatusReasonSuccess)
				message = "source is instrumented and healthy, no telemetry recorded yet"
			}
		} else {
			reasonStr = string(status.WorkloadOdigosHealthStatusReasonSuccess)
			message = "source is healthy, no telemetry is expected"
		}
		return &model.DesiredConditionStatus{
			Name:       status.WorkloadOdigosHealthStatus,
			Status:     model.DesiredStateProgressSuccess,
			ReasonEnum: &reasonStr,
			Message:    message,
		}, nil
	}

	return mostSevereCondition, nil
}

// Conditions is the resolver for the conditions field.
func (r *k8sWorkloadResolver) Conditions(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadConditions, error) {
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	runtimeDetection := status.CalculateRuntimeInspectionStatus(ic)
	agentInjectionEnabled := status.CalculateAgentInjectionEnabledStatus(ic)
	rollout := status.CalculateRolloutStatus(ic)
	agentInjected := status.CalculateAgentInjectedStatus(ic, pods)
	processesAgentHealth, err := aggregateProcessesHealthForWorkload(ctx, obj.ID)
	if err != nil {
		return nil, err
	}
	telemetryMetrics := status.CalculateExpectingTelemetryStatus(ic, pods, nil)

	return &model.K8sWorkloadConditions{
		RuntimeDetection:      runtimeDetection,
		AgentInjectionEnabled: agentInjectionEnabled,
		Rollout:               rollout,
		AgentInjected:         agentInjected,
		ProcessesAgentHealth:  processesAgentHealth,
		ExpectingTelemetry:    telemetryMetrics.TelemetryObservedStatus,
	}, nil
}

// MarkedForInstrumentation is the resolver for the markedForInstrumentation field.
func (r *k8sWorkloadResolver) MarkedForInstrumentation(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadMarkedForInstrumentation, error) {
	l := loaders.For(ctx)
	sources, err := l.GetSources(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	enabled, reason, err := sourceutils.IsObjectInstrumentedBySource(ctx, sources, err)
	if err != nil {
		return nil, err
	}

	var markedForInstrumentation *bool
	if enabled {
		markedForInstrumentation = &enabled
	} else {
		if reason.Reason == string(odigosv1.MarkedForInstrumentationReasonWorkloadSourceDisabled) {
			markedForInstrumentation = &enabled
		}
	}

	return &model.K8sWorkloadMarkedForInstrumentation{
		MarkedForInstrumentation: markedForInstrumentation,
		DecisionEnum:             string(reason.Reason),
		Message:                  reason.Message,
	}, nil
}

// RuntimeInfo is the resolver for the runtimeInfo field.
func (r *k8sWorkloadResolver) RuntimeInfo(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadRuntimeInfo, error) {
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	completed := len(ic.Status.RuntimeDetailsByContainer) > 0

	uniqueLanguages := make(map[common.ProgrammingLanguage]struct{})
	containers := make([]*model.K8sWorkloadRuntimeInfoContainer, len(ic.Status.RuntimeDetailsByContainer))
	for i, container := range ic.Status.RuntimeDetailsByContainer {
		containers[i] = runtimeDetailsContainersToModel(&container)
		_, ignored := l.GetIgnoredContainers()[container.ContainerName]
		if container.Language != common.UnknownProgrammingLanguage && !ignored {
			uniqueLanguages[container.Language] = struct{}{}
		}
	}
	var detectedLanguages []model.ProgrammingLanguage
	if completed {
		detectedLanguages = make([]model.ProgrammingLanguage, 0, len(uniqueLanguages))
		for language := range uniqueLanguages {
			detectedLanguages = append(detectedLanguages, model.ProgrammingLanguage(language))
		}
	}

	runtimeInfo := &model.K8sWorkloadRuntimeInfo{
		Completed:         completed,
		CompletedStatus:   status.CalculateRuntimeInspectionStatus(ic),
		DetectedLanguages: detectedLanguages,
		Containers:        containers,
	}

	return runtimeInfo, nil
}

// AgentEnabled is the resolver for the agentEnabled field.
func (r *k8sWorkloadResolver) AgentEnabled(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadAgentEnabled, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	containers := make([]*model.K8sWorkloadAgentEnabledContainer, 0, len(ic.Spec.Containers))
	for _, container := range ic.Spec.Containers {
		containerModel := agentEnabledContainersToModel(&container)
		containers = append(containers, containerModel)
	}

	return &model.K8sWorkloadAgentEnabled{
		AgentEnabled:  ic.Spec.AgentInjectionEnabled,
		EnabledStatus: status.CalculateAgentInjectionEnabledStatus(ic),
		Containers:    containers,
	}, nil
}

// Rollout is the resolver for the rollout field.
func (r *k8sWorkloadResolver) Rollout(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadRollout, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	rolloutStatus := status.CalculateRolloutStatus(ic)
	if rolloutStatus == nil {
		return nil, nil
	}

	return &model.K8sWorkloadRollout{
		RolloutStatus: rolloutStatus,
	}, nil
}

// Containers is the resolver for the containers field.
func (r *k8sWorkloadResolver) Containers(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadContainer, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	containerByName := make(map[string]*model.K8sWorkloadContainer)
	for _, container := range ic.Spec.Containers {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		containerByName[container.ContainerName].AgentEnabled = agentEnabledContainersToModel(&container)
	}

	for _, container := range ic.Status.RuntimeDetailsByContainer {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		containerByName[container.ContainerName].RuntimeInfo = runtimeDetailsContainersToModel(&container)
	}

	for _, container := range ic.Spec.ContainersOverrides {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		overrides := &model.K8sWorkloadContainerOverrides{
			ContainerName: container.ContainerName,
		}
		if container.RuntimeInfo != nil {
			overrides.RuntimeInfo = runtimeDetailsContainersToModel(container.RuntimeInfo)
		}
		containerByName[container.ContainerName].Overrides = overrides
	}

	containers := make([]*model.K8sWorkloadContainer, 0, len(containerByName))
	for _, container := range containerByName {
		containers = append(containers, container)
	}

	return containers, nil
}

// Pods is the resolver for the pods field.
func (r *k8sWorkloadResolver) Pods(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadPod, error) {
	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	podModels := make([]*model.K8sWorkloadPod, 0, len(pods))
	for _, pod := range pods {
		containerModels := make([]*model.K8sWorkloadPodContainer, 0, len(pod.Containers))

		containersHealthConditions := make([]*model.DesiredConditionStatus, 0, len(pod.Containers))
		for _, container := range pod.Containers {
			healthStatus := status.CalculatePodContainerHealthStatus(&container)
			containersHealthConditions = append(containersHealthConditions, healthStatus)
			containerModels = append(containerModels, &model.K8sWorkloadPodContainer{
				ContainerName:                   container.ContainerName,
				OtelDistroName:                  container.OtelDistroName,
				OdigosInstrumentationDeviceName: container.OdigosInstrumentationDeviceName,
				Started:                         container.Started,
				Ready:                           container.Ready,
				IsCrashLoop:                     &container.IsCrashLoop,
				RestartCount:                    container.RestartCount,
				RunningStartedTime:              container.RunningStartedTime,
				WaitingReasonEnum:               container.WaitingReasonEnum,
				WaitingMessage:                  container.WaitingMessage,
				HealthStatus:                    healthStatus,
			})
		}

		podHealthStatus := aggregateConditionsBySeverity(containersHealthConditions)
		if podHealthStatus == nil {
			// should not happen, all containers health status should be calculated.
			reasonStr := string(status.PodContainerHealthReasonUnknown)
			podHealthStatus = &model.DesiredConditionStatus{
				Name:       status.PodHealthStatus,
				Status:     model.DesiredStateProgressError,
				ReasonEnum: &reasonStr,
				Message:    "not able to determine health status for containers in pod",
			}
		}
		if podHealthStatus.ReasonEnum != nil {
			// set a better message for the pod health condition
			if *podHealthStatus.ReasonEnum == string(status.PodContainerHealthReasonHealthy) {
				podHealthStatus.Message = "all containers in pod are reported healthy in kubernetes"
			} else if *podHealthStatus.ReasonEnum == string(status.PodContainerHealthReasonNotStarted) {
				podHealthStatus.Message = "some containers in pod are not started yet"
			} else if *podHealthStatus.ReasonEnum == string(status.PodContainerHealthReasonNotReady) {
				podHealthStatus.Message = "some containers in pod are not ready yet"
			} else if *podHealthStatus.ReasonEnum == string(status.PodContainerHealthReasonCrashLoopBackOff) {
				podHealthStatus.Message = "some containers in pod are in crash loop back off"
			}
		}

		podModels = append(podModels, &model.K8sWorkloadPod{
			PodName:             pod.PodName,
			NodeName:            pod.PodNodeName,
			StartTime:           pod.PodStartTime,
			AgentInjected:       pod.AgentInjected,
			AgentInjectedStatus: pod.AgentInjectedStatus,
			// TODO: RunningLatestWorkloadRevision
			Containers:      containerModels,
			PodHealthStatus: podHealthStatus,
		})
	}
	return podModels, nil
}

// PodsAgentInjectionStatus is the resolver for the podsAgentInjectionStatus field.
func (r *k8sWorkloadResolver) PodsAgentInjectionStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}
	instrumentationConfig, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	agentInjectionStatus := status.CalculateAgentInjectedStatus(instrumentationConfig, pods)
	return agentInjectionStatus, nil
}

// PodsHealthStatus is the resolver for the podsHealthStatus field.
func (r *k8sWorkloadResolver) PodsHealthStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	// aggregate the health status of all containers in all pods
	containersHealthConditions := make([]*model.DesiredConditionStatus, 0, len(pods))
	for _, pod := range pods {
		for _, container := range pod.Containers {
			healthStatus := status.CalculatePodContainerHealthStatus(&container)
			containersHealthConditions = append(containersHealthConditions, healthStatus)
		}
	}

	aggregatePodHealthStatus := aggregateConditionsBySeverity(containersHealthConditions)
	if aggregatePodHealthStatus == nil {
		reasonStr := string(status.PodContainerHealthReasonUnknown)
		return &model.DesiredConditionStatus{
			Name:       status.PodHealthStatus,
			Status:     model.DesiredStateProgressError,
			ReasonEnum: &reasonStr,
			Message:    "not able to determine health status for containers in pods",
		}, nil
	}
	if aggregatePodHealthStatus.ReasonEnum != nil {
		if *aggregatePodHealthStatus.ReasonEnum == string(status.PodContainerHealthReasonHealthy) {
			aggregatePodHealthStatus.Message = "all containers in all pods are reported healthy in kubernetes"
		} else if *aggregatePodHealthStatus.ReasonEnum == string(status.PodContainerHealthReasonNotStarted) {
			aggregatePodHealthStatus.Message = "some containers in this workload's pods are not started yet"
		} else if *aggregatePodHealthStatus.ReasonEnum == string(status.PodContainerHealthReasonNotReady) {
			aggregatePodHealthStatus.Message = "some containers in this workload's pods are not ready yet"
		} else if *aggregatePodHealthStatus.ReasonEnum == string(status.PodContainerHealthReasonCrashLoopBackOff) {
			aggregatePodHealthStatus.Message = "some containers in this workload's pods are in crash loop back off"
		}
	}
	return aggregatePodHealthStatus, nil
}

// WorkloadHealthStatus is the resolver for the workloadHealthStatus field.
func (r *k8sWorkloadResolver) WorkloadHealthStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	l := loaders.For(ctx)
	workload, err := l.GetWorkloadManifest(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	if workload == nil {
		return nil, nil
	}

	// WorkloadHealthStatus may also be nil here which is ok
	return workload.WorkloadHealthStatus, nil
}

// ProcessesHealthStatus is the resolver for the processesHealthStatus field.
func (r *k8sWorkloadResolver) ProcessesHealthStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	return aggregateProcessesHealthForWorkload(ctx, obj.ID)
}

// TelemetryMetrics is the resolver for the telemetryMetrics field.
func (r *k8sWorkloadResolver) TelemetryMetrics(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadTelemetryMetrics, error) {
	var totalDataSent *int
	var throughput *int

	// attempt to get the metrics for the workload, or keep them as nil if not available.
	workloadMetrics, ok := r.MetricsConsumer.GetSingleSourceMetrics(frontendcommon.SourceID{
		Namespace: obj.ID.Namespace,
		Kind:      k8sconsts.WorkloadKind(obj.ID.Kind),
		Name:      obj.ID.Name,
	})
	if ok {
		tds := int(workloadMetrics.TotalDataSent())
		tp := int(workloadMetrics.TotalThroughput())
		totalDataSent = &tds
		throughput = &tp
	}

	return []*model.K8sWorkloadTelemetryMetrics{
		{
			TotalDataSentBytes: totalDataSent,
			ThroughputBytes:    throughput,
		},
	}, nil
}

// Processes is the resolver for the processes field.
func (r *k8sWorkloadPodContainerResolver) Processes(ctx context.Context, obj *model.K8sWorkloadPodContainer) ([]*model.K8sWorkloadPodContainerProcess, error) {
	l := loaders.For(ctx)

	containerName := obj.ContainerName

	// extract pod info from the parent resolver context
	fc := graphql.GetFieldContext(ctx)
	if fc == nil || fc.Parent == nil || fc.Parent.Parent == nil || fc.Parent.Parent.Parent == nil {
		return nil, fmt.Errorf("missisng parent resolver context")
	}
	workloadPodCtx := fc.Parent.Parent.Parent
	p, ok := workloadPodCtx.Result.(**model.K8sWorkloadPod)
	if !ok || p == nil || (*p).PodName == "" {
		return nil, fmt.Errorf("parent is not a pod")
	}
	podName := (*p).PodName

	if workloadPodCtx.Parent == nil || workloadPodCtx.Parent.Parent == nil {
		return nil, fmt.Errorf("missing parent resolver context")
	}
	workloadCtx := workloadPodCtx.Parent.Parent
	c, ok := workloadCtx.Result.(**model.K8sWorkload)
	if !ok || c == nil || (*c).ID == nil {
		return nil, fmt.Errorf("parent is not a workload")
	}
	workloadId := *(*c).ID

	instrumentationInstances, err := l.GetInstrumentationInstancesForContainer(ctx, loaders.ContainerId{
		Namespace:     workloadId.Namespace,
		PodName:       podName,
		ContainerName: containerName,
	})
	if err != nil {
		return nil, err
	}

	processes := make([]*model.K8sWorkloadPodContainerProcess, 0, len(instrumentationInstances))
	for _, instrumentationInstance := range instrumentationInstances {
		processHealthStatus := status.CalculateProcessHealthStatus(instrumentationInstance)
		identifyingAttributes := make([]*model.K8sWorkloadPodContainerProcessAttribute, 0, len(instrumentationInstance.Status.IdentifyingAttributes))
		for _, attribute := range instrumentationInstance.Status.IdentifyingAttributes {
			identifyingAttributes = append(identifyingAttributes, &model.K8sWorkloadPodContainerProcessAttribute{
				Name:  attribute.Key,
				Value: attribute.Value,
			})
		}
		processes = append(processes, &model.K8sWorkloadPodContainerProcess{
			Healthy:               instrumentationInstance.Status.Healthy,
			HealthStatus:          processHealthStatus,
			IdentifyingAttributes: identifyingAttributes,
		})
	}
	return processes, nil
}

// ExpectingTelemetry is the resolver for the expectingTelemetry field.
func (r *k8sWorkloadTelemetryMetricsResolver) ExpectingTelemetry(ctx context.Context, obj *model.K8sWorkloadTelemetryMetrics) (*model.K8sWorkloadTelemetryMetricsExpectingTelemetryStatus, error) {
	// Safely derive the parent workload ID from the GraphQL field context
	var workloadId model.K8sWorkloadID
	fc := graphql.GetFieldContext(ctx)
	// go up 3 levels in the context to get the workload id
	// current field -> *model.K8sWorkloadTelemetryMetrics -> []*model.K8sWorkloadTelemetryMetrics -> *model.K8sWorkload
	if fc == nil || fc.Parent == nil || fc.Parent.Parent == nil || fc.Parent.Parent.Parent == nil {
		return nil, fmt.Errorf("missisng parent resolver context")
	}
	w, ok := fc.Parent.Parent.Parent.Result.(**model.K8sWorkload)
	if !ok || w == nil || (*w).ID == nil {
		return nil, fmt.Errorf("parent is not a workload")
	}
	workloadId = *(*w).ID

	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, workloadId)
	if err != nil {
		return nil, err
	}
	ic, err := l.GetInstrumentationConfig(ctx, workloadId)
	if err != nil {
		return nil, err
	}

	return status.CalculateExpectingTelemetryStatus(ic, pods, obj.TotalDataSentBytes), nil
}

// K8sWorkload returns K8sWorkloadResolver implementation.
func (r *Resolver) K8sWorkload() K8sWorkloadResolver { return &k8sWorkloadResolver{r} }

// K8sWorkloadPodContainer returns K8sWorkloadPodContainerResolver implementation.
func (r *Resolver) K8sWorkloadPodContainer() K8sWorkloadPodContainerResolver {
	return &k8sWorkloadPodContainerResolver{r}
}

// K8sWorkloadTelemetryMetrics returns K8sWorkloadTelemetryMetricsResolver implementation.
func (r *Resolver) K8sWorkloadTelemetryMetrics() K8sWorkloadTelemetryMetricsResolver {
	return &k8sWorkloadTelemetryMetricsResolver{r}
}

type k8sWorkloadResolver struct{ *Resolver }
type k8sWorkloadPodContainerResolver struct{ *Resolver }
type k8sWorkloadTelemetryMetricsResolver struct{ *Resolver }

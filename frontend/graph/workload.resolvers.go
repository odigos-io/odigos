package graph

// This file will be automatically regenerated based on the schema, any resolver implementations
// will be copied through when generating and any unknown code will be moved to the end.
// Code generated by github.com/99designs/gqlgen version v0.17.70

import (
	"context"
	"fmt"
	"time"

	"github.com/99designs/gqlgen/graphql"
	"github.com/odigos-io/odigos/api/k8sconsts"
	"github.com/odigos-io/odigos/frontend/graph/loaders"
	"github.com/odigos-io/odigos/frontend/graph/model"
	"github.com/odigos-io/odigos/frontend/graph/status"
	"github.com/odigos-io/odigos/frontend/services/common"
	sourceutils "github.com/odigos-io/odigos/k8sutils/pkg/source"
)

// WorkloadOdigosHealthStatus is the resolver for the workloadOdigosHealthStatus field.
func (r *k8sWorkloadResolver) WorkloadOdigosHealthStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	conditions := []*model.DesiredConditionStatus{}
	conditions = append(conditions, status.CalculateRuntimeInspectionStatus(ic))
	conditions = append(conditions, status.CalculateAgentInjectionEnabledStatus(ic))
	conditions = append(conditions, status.CalculateRolloutStatus(ic))

	allSuccess := true
	for _, condition := range conditions {
		if condition == nil {
			continue
		}
		if condition.Status != model.DesiredStateProgressSuccess {
			allSuccess = false
			break
		}
	}

	if allSuccess {
		reasonStr := string(status.WorkloadOdigosHealthStatusReasonSuccess)
		return &model.DesiredConditionStatus{
			Name:       status.WorkloadOdigosHealthStatus,
			Status:     model.DesiredStateProgressSuccess,
			ReasonEnum: &reasonStr,
			Message:    "all odigos conditions are successful",
		}, nil
	}

	reasonStr := string(status.WorkloadOdigosHealthStatusReasonError)
	return &model.DesiredConditionStatus{
		Name:       status.WorkloadOdigosHealthStatus,
		Status:     model.DesiredStateProgressError,
		ReasonEnum: &reasonStr,
		Message:    "some odigos conditions are not successful",
	}, nil
}

// MarkedForInstrumentation is the resolver for the markedForInstrumentation field.
func (r *k8sWorkloadResolver) MarkedForInstrumentation(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadMakredForInstrumentation, error) {
	l := loaders.For(ctx)
	sources, err := l.GetSources(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	enabled, reason, err := sourceutils.IsObjectInstrumentedBySource(ctx, sources, err)
	if err != nil {
		return nil, err
	}

	return &model.K8sWorkloadMakredForInstrumentation{
		MarkedForInstrumentation: enabled,
		DecisionEnum:             string(reason.Reason),
		Message:                  reason.Message,
	}, nil
}

// RuntimeInfo is the resolver for the runtimeInfo field.
func (r *k8sWorkloadResolver) RuntimeInfo(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadRuntimeInfo, error) {
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	containers := make([]*model.K8sWorkloadRuntimeInfoContainer, len(ic.Status.RuntimeDetailsByContainer))
	for i, container := range ic.Status.RuntimeDetailsByContainer {
		containers[i] = runtimeDetailsContainersToModel(&container)
	}

	runtimeInfo := &model.K8sWorkloadRuntimeInfo{
		Completed:       len(ic.Status.RuntimeDetailsByContainer) > 0,
		CompletedStatus: status.CalculateRuntimeInspectionStatus(ic),
		Containers:      containers,
	}

	return runtimeInfo, nil
}

// AgentEnabled is the resolver for the agentEnabled field.
func (r *k8sWorkloadResolver) AgentEnabled(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadAgentEnabled, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	containers := make([]*model.K8sWorkloadAgentEnabledContainer, 0, len(ic.Spec.Containers))
	for _, container := range ic.Spec.Containers {
		containerModel := agentEnabledContainersToModel(&container)
		containers = append(containers, containerModel)
	}

	return &model.K8sWorkloadAgentEnabled{
		AgentEnabled:  ic.Spec.AgentInjectionEnabled,
		EnabledStatus: status.CalculateAgentInjectionEnabledStatus(ic),
		Containers:    containers,
	}, nil
}

// Rollout is the resolver for the rollout field.
func (r *k8sWorkloadResolver) Rollout(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadRollout, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	rolloutStatus := status.CalculateRolloutStatus(ic)
	if rolloutStatus == nil {
		return nil, nil
	}

	return &model.K8sWorkloadRollout{
		RolloutStatus: rolloutStatus,
	}, nil
}

// Containers is the resolver for the containers field.
func (r *k8sWorkloadResolver) Containers(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadContainer, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	containerByName := make(map[string]*model.K8sWorkloadContainer)
	for _, container := range ic.Spec.Containers {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		containerByName[container.ContainerName].AgentEnabled = agentEnabledContainersToModel(&container)
	}

	for _, container := range ic.Status.RuntimeDetailsByContainer {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		containerByName[container.ContainerName].RuntimeInfo = runtimeDetailsContainersToModel(&container)
	}

	for _, container := range ic.Spec.ContainersOverrides {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		overrides := &model.K8sWorkloadContainerOverrides{
			ContainerName: container.ContainerName,
		}
		if container.RuntimeInfo != nil {
			overrides.RuntimeInfo = runtimeDetailsContainersToModel(container.RuntimeInfo)
		}
		containerByName[container.ContainerName].Overrides = overrides
	}

	containers := make([]*model.K8sWorkloadContainer, 0, len(containerByName))
	for _, container := range containerByName {
		containers = append(containers, container)
	}

	return containers, nil
}

// Pods is the resolver for the pods field.
func (r *k8sWorkloadResolver) Pods(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadPod, error) {
	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	podModels := make([]*model.K8sWorkloadPod, 0, len(pods))
	for _, cachePod := range pods {
		containerModels := make([]*model.K8sWorkloadPodContainer, 0, len(cachePod.Pod.Spec.Containers))

		// set aggregated pod health status to success and override if any container is not healthy.
		podHealthReasonStr := string(PodContainerHealthReasonHealthy)
		podHealthStatus := &model.DesiredConditionStatus{
			Name:       podHealthStatus,
			Status:     model.DesiredStateProgressSuccess,
			ReasonEnum: &podHealthReasonStr,
			Message:    "all containers are healthy",
		}
		for _, container := range cachePod.Pod.Spec.Containers {
			containerStatus := getContainerStatus(cachePod.Pod, container.Name)
			var started, ready *bool
			var isCrashLoop bool = false // never set to nil
			var runninsStartedTime, waitingReasonEnum, waitingMessage *string
			var restartCount *int
			if containerStatus != nil {
				started = containerStatus.Started
				ready = &containerStatus.Ready
				restartCountInt := int(containerStatus.RestartCount)
				restartCount = &restartCountInt
				if containerStatus.State.Waiting != nil {
					isCrashLoop = containerStatus.State.Waiting.Reason == "CrashLoopBackOff"
					waitingReasonEnum = &containerStatus.State.Waiting.Reason
					waitingMessage = &containerStatus.State.Waiting.Message
				}
				if containerStatus.State.Running != nil {
					runningStartedTimeStr := containerStatus.State.Running.StartedAt.Format(time.RFC3339)
					runninsStartedTime = &runningStartedTimeStr
				}
			}

			// pod container is considered healthy if it is started, ready and not in crash loop back off.
			healthStatus := &model.DesiredConditionStatus{
				Name: podContainerHealthStatus,
			}
			if isCrashLoop {
				reasonStr := string(PodContainerHealthReasonCrashLoopBackOff)

				healthStatus.Status = model.DesiredStateProgressError
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod in crash loop back off: " + *waitingReasonEnum

				podHealthStatus.Status = model.DesiredStateProgressError
				podHealthStatus.ReasonEnum = &reasonStr
				podHealthStatus.Message = "container in pod is in crash loop back off"

			} else if started == nil || !*started {
				reasonStr := string(PodContainerHealthReasonNotStarted)

				healthStatus.Status = model.DesiredStateProgressWaiting
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod not started yet"

				if podHealthStatus.Status != model.DesiredStateProgressError {
					podHealthStatus.Status = model.DesiredStateProgressWaiting
					podHealthStatus.ReasonEnum = &reasonStr
					podHealthStatus.Message = "container in pod is not started yet"
				}

			} else if ready == nil || !*ready {
				reasonStr := string(PodContainerHealthReasonNotReady)

				healthStatus.Status = model.DesiredStateProgressWaiting
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod not ready yet"

				if podHealthStatus.Status != model.DesiredStateProgressError {
					podHealthStatus.Status = model.DesiredStateProgressWaiting
					podHealthStatus.ReasonEnum = &reasonStr
					podHealthStatus.Message = "container in pod is not ready yet"
				}
			} else {
				reasonStr := string(PodContainerHealthReasonHealthy)
				healthStatus.Status = model.DesiredStateProgressSuccess
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod is healthy"
			}

			containerModels = append(containerModels, &model.K8sWorkloadPodContainer{
				ContainerName:      container.Name,
				Started:            started,
				Ready:              ready,
				IsCrashLoop:        &isCrashLoop,
				RestartCount:       restartCount,
				RunningStartedTime: runninsStartedTime,
				WaitingReasonEnum:  waitingReasonEnum,
				WaitingMessage:     waitingMessage,
				HealthStatus:       healthStatus,
			})
		}
		podModels = append(podModels, &model.K8sWorkloadPod{
			PodName:             cachePod.Pod.Name,
			NodeName:            cachePod.Pod.Spec.NodeName,
			StartTime:           cachePod.Pod.CreationTimestamp.Format(time.RFC3339),
			AgentInjected:       cachePod.ComputedPodValues.AgentInjected,
			AgentInjectedStatus: cachePod.ComputedPodValues.AgentInjectedStatus,
			// TODO: RunningLatestWorkloadRevision
			Containers:      containerModels,
			PodHealthStatus: podHealthStatus,
		})
	}
	return podModels, nil
}

// PodsAgentInjectionStatus is the resolver for the podsAgentInjectionStatus field.
func (r *k8sWorkloadResolver) PodsAgentInjectionStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}
	instrumentationConfig, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	if len(pods) == 0 {
		reasonStr := string(PodsAgentInjectionReasonNoPodsAgentInjected)
		return &model.DesiredConditionStatus{
			Name:       status.AgentInjectedStatus,
			Status:     model.DesiredStateProgressDisabled,
			ReasonEnum: &reasonStr,
			Message:    "no pods found for this workload",
		}, nil
	}

	numSuccess := 0
	numNotSuccess := 0
	for _, pod := range pods {
		if pod.ComputedPodValues.AgentInjectedStatus.Status == model.DesiredStateProgressSuccess {
			numSuccess++
		} else {
			numNotSuccess++
		}
	}

	// if instrumentationConfig is nil, we assume agent is not enabled.
	agentEnabled := false
	if instrumentationConfig != nil {
		agentEnabled = instrumentationConfig.Spec.AgentInjectionEnabled
	}

	if numSuccess == 0 && numNotSuccess > 0 {
		var reasonStr, message string
		if agentEnabled {
			reasonStr = string(PodsAgentInjectionReasonSomePodsAgentInjected)
			message = fmt.Sprintf("%d/%d pods have agent injected when it should not", numNotSuccess, len(pods))
		} else {
			reasonStr = string(PodsAgentInjectionReasonSomePodsAgentNotInjected)
			message = fmt.Sprintf("%d/%d pods do not have agent injected when it should", numNotSuccess, len(pods))
		}
		return &model.DesiredConditionStatus{
			Name:       status.AgentInjectedStatus,
			Status:     model.DesiredStateProgressWaiting,
			ReasonEnum: &reasonStr,
			Message:    message,
		}, nil
	} else {
		var reasonStr, message string
		if agentEnabled {
			reasonStr = string(PodsAgentInjectionReasonAllPodsAgentInjected)
			message = fmt.Sprintf("all %d pods have odigos agent injected as expected", numSuccess)
		} else {
			reasonStr = string(PodsAgentInjectionReasonAllPodsAgentNotInjected)
			message = fmt.Sprintf("all %d pods do not have odigos agent injected as expected", numSuccess)
		}
		return &model.DesiredConditionStatus{
			Name:       status.AgentInjectedStatus,
			Status:     model.DesiredStateProgressSuccess,
			ReasonEnum: &reasonStr,
			Message:    message,
		}, nil
	}
}

// TelemetryMetrics is the resolver for the telemetryMetrics field.
func (r *k8sWorkloadResolver) TelemetryMetrics(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadTelemetryMetrics, error) {
	var totalDataSent *int
	var throughput *int

	// attempt to get the metrics for the workload, or keep them as nil if not available.
	workloadMetrics, ok := r.MetricsConsumer.GetSingleSourceMetrics(common.SourceID{
		Namespace: obj.ID.Namespace,
		Kind:      k8sconsts.WorkloadKind(obj.ID.Kind),
		Name:      obj.ID.Name,
	})
	if ok {
		tds := int(workloadMetrics.TotalDataSent())
		tp := int(workloadMetrics.TotalThroughput())
		totalDataSent = &tds
		throughput = &tp
	}

	return []*model.K8sWorkloadTelemetryMetrics{
		{
			TotalDataSentBytes: totalDataSent,
			ThroughputBytes:    throughput,
		},
	}, nil
}

// ExpectingTelemetry is the resolver for the expectingTelemetry field.
func (r *k8sWorkloadTelemetryMetricsResolver) ExpectingTelemetry(ctx context.Context, obj *model.K8sWorkloadTelemetryMetrics) (*model.K8sWorkloadTelemetryMetricsExpectingTelemetryStatus, error) {
	// Safely derive the parent workload ID from the GraphQL field context
	var workloadId model.K8sWorkloadID
	fc := graphql.GetFieldContext(ctx)
	// go up 3 levels in the context to get the workload id
	// current field -> *model.K8sWorkloadTelemetryMetrics -> []*model.K8sWorkloadTelemetryMetrics -> *model.K8sWorkload
	if fc == nil || fc.Parent == nil || fc.Parent.Parent == nil || fc.Parent.Parent.Parent == nil {
		return nil, fmt.Errorf("missisng parent resolver context")
	}
	w, ok := fc.Parent.Parent.Parent.Result.(**model.K8sWorkload)
	if !ok || w == nil || (*w).ID == nil {
		return nil, fmt.Errorf("parent is not a workload")
	}
	workloadId = *(*w).ID

	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, workloadId)
	if err != nil {
		return nil, err
	}
	ic, err := l.GetInstrumentationConfig(ctx, workloadId)
	if err != nil {
		return nil, err
	}

	expectingTelemetry := false

	// at the moment, a workload is expected to have telemetry
	// if the workload has agent injection enabled and at least one pod has the agent injected.
	if ic == nil {
		reasonStr := string(status.ExpectingTelemetryReasonWorkloadNotMarkedForInstrumentation)
		return &model.K8sWorkloadTelemetryMetricsExpectingTelemetryStatus{
			IsExpectingTelemetry: &expectingTelemetry,
			TelemetryObservedStatus: &model.DesiredConditionStatus{
				Name:       status.ExpectingTelemetryStatus,
				Status:     model.DesiredStateProgressDisabled,
				ReasonEnum: &reasonStr,
				Message:    "workload is not marked for instrumentation",
			},
		}, nil
	}

	if !ic.Spec.AgentInjectionEnabled {
		reasonStr := string(status.ExpectingTelemetryReasonAgentNotEnabledForInjection)
		return &model.K8sWorkloadTelemetryMetricsExpectingTelemetryStatus{
			IsExpectingTelemetry: &expectingTelemetry,
			TelemetryObservedStatus: &model.DesiredConditionStatus{
				Name:       status.ExpectingTelemetryStatus,
				Status:     model.DesiredStateProgressDisabled,
				ReasonEnum: &reasonStr,
				Message:    "agent injection is not enabled for this workload",
			},
		}, nil
	}

	if len(pods) == 0 {
		reasonStr := string(status.ExpectingTelemetryReasonAgentNoRunningPod)
		return &model.K8sWorkloadTelemetryMetricsExpectingTelemetryStatus{
			IsExpectingTelemetry: &expectingTelemetry,
			TelemetryObservedStatus: &model.DesiredConditionStatus{
				Name:       status.ExpectingTelemetryStatus,
				Status:     model.DesiredStateProgressDisabled,
				ReasonEnum: &reasonStr,
				Message:    "no running pods found for this workload",
			},
		}, nil
	}

	for _, pod := range pods {
		if pod.ComputedPodValues.AgentInjected {
			expectingTelemetry = true
			break
		}
	}

	if !expectingTelemetry {
		reasonStr := string(status.ExpectingTelemetryReasonAgentNotInjected)
		return &model.K8sWorkloadTelemetryMetricsExpectingTelemetryStatus{
			IsExpectingTelemetry: &expectingTelemetry,
			TelemetryObservedStatus: &model.DesiredConditionStatus{
				Name:       status.ExpectingTelemetryStatus,
				Status:     model.DesiredStateProgressWaiting,
				ReasonEnum: &reasonStr,
				Message:    "agent is not injected into any of the pods in this workload",
			},
		}, nil
	}

	if obj.TotalDataSentBytes == nil || *obj.TotalDataSentBytes == 0 {
		reasonStr := string(status.ExpectingTelemetryReasonAgentInjectedButNoDataSent)
		return &model.K8sWorkloadTelemetryMetricsExpectingTelemetryStatus{
			IsExpectingTelemetry: &expectingTelemetry,
			TelemetryObservedStatus: &model.DesiredConditionStatus{
				Name:       status.ExpectingTelemetryStatus,
				Status:     model.DesiredStateProgressWaiting,
				ReasonEnum: &reasonStr,
				Message:    "agent is injected into the pods in this workload, but no telemetry data was sent yet",
			},
		}, nil
	}

	reasonStr := string(status.ExpectingTelemetryReasonAgentInjectedAndDataSent)
	return &model.K8sWorkloadTelemetryMetricsExpectingTelemetryStatus{
		IsExpectingTelemetry: &expectingTelemetry,
		TelemetryObservedStatus: &model.DesiredConditionStatus{
			Name:       status.ExpectingTelemetryStatus,
			Status:     model.DesiredStateProgressSuccess,
			ReasonEnum: &reasonStr,
			Message:    "agent is injected into the pods in this workload, and telemetry data was sent",
		},
	}, nil
}

// K8sWorkload returns K8sWorkloadResolver implementation.
func (r *Resolver) K8sWorkload() K8sWorkloadResolver { return &k8sWorkloadResolver{r} }

// K8sWorkloadTelemetryMetrics returns K8sWorkloadTelemetryMetricsResolver implementation.
func (r *Resolver) K8sWorkloadTelemetryMetrics() K8sWorkloadTelemetryMetricsResolver {
	return &k8sWorkloadTelemetryMetricsResolver{r}
}

type k8sWorkloadResolver struct{ *Resolver }
type k8sWorkloadTelemetryMetricsResolver struct{ *Resolver }
